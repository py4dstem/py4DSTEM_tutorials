{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5586b0-2375-4645-b70c-73691a9fa53d",
   "metadata": {},
   "source": [
    "# Ptychography Tutorial 04\n",
    "\n",
    "This is the fourth tutorial notebook in the iterative ptychography series.  \n",
    "In this tutorial notebook we will cover:\n",
    "- GPU memory-management for ptychographic reconstructions\n",
    "\n",
    "### Downloads\n",
    "This tutorial uses the following datasets:\n",
    "- [ptycho_ducky_simulation_01.h5](https://drive.google.com/file/d/1VtYVHWiuI8AT0yucaylIcQQJ_px79ash/view?usp=drive_link)\n",
    "- [ptycho_ducky_vacuum-probe_01.h5](https://drive.google.com/file/d/1CeVGVvMf2QeJK1ADQ9MBK308NhvoVaVn/view?usp=drive_link)\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "This tutorial was created by the py4DSTEM `phase_contrast` team:\n",
    "- Georgios Varnavides (gvarnavides@berkeley.edu)\n",
    "- Stephanie Ribet (sribet@lbl.gov)\n",
    "- Colin Ophus (clophus@lbl.gov)\n",
    "\n",
    "Last updated: 2024 May 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c213cb-bc06-44b4-9c20-688763e2750a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Ptychographic reconstructions can be very computationally expensive. py4DSTEM offers single-node GPU acceleration to speed up these calculations.\n",
    "\n",
    "GPU calculations however, are often VRAM (memory) limited. Here, we investigate various flags as well as tips and tricks to enable efficient GPU memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4b2849-740e-4175-bfe3-0b60f04fe15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.14\n",
      "13.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import py4DSTEM\n",
    "\n",
    "from py4DSTEM.process.phase.utils import get_array_module\n",
    "from cupy.fft.config import get_plan_cache\n",
    "\n",
    "print(py4DSTEM.__version__)\n",
    "print(cp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66582ca0-0f80-4df2-8151-aa6e357f3e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCube( A 4-dimensional array of shape (33, 33, 200, 200) called 'datacube',\n",
       "          with dimensions:\n",
       "\n",
       "              Rx = [0.0,5.0,10.0,...] A\n",
       "              Ry = [0.0,5.0,10.0,...] A\n",
       "              Qx = [0.0,0.025,0.05,...] A^-1\n",
       "              Qy = [0.0,0.025,0.05,...] A^-1\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data/'\n",
    "file_data = file_path + 'ptycho_ducky_simulation_01.h5'\n",
    "file_probe = file_path + 'ptycho_ducky_vacuum-probe_01.h5'\n",
    "\n",
    "probe = py4DSTEM.read(file_probe)\n",
    "dataset = py4DSTEM.read(file_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5eb1f-a039-415b-b354-0120a4fcce3b",
   "metadata": {},
   "source": [
    "### GPU Memory Usage\n",
    "\n",
    "First, let's define two helper functions to:  \n",
    "- clear a class's cupy arrays\n",
    "- report class GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2978732-4a12-43ce-aaf2-b4483f96e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mempool = cp.get_default_memory_pool()\n",
    "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "cache = get_plan_cache()\n",
    "\n",
    "def report_class_gpu_memory_usage(cls):\n",
    "\n",
    "    size_on_device = 0\n",
    "    for att in sorted(cls.__dict__):\n",
    "        attr = getattr(cls,att)\n",
    "        \n",
    "        if isinstance(attr,(np.ndarray,cp.ndarray)):\n",
    "            \n",
    "            if get_array_module(attr) is not np:\n",
    "                size_on_device += attr.nbytes\n",
    "                print(f\"Attribute {att}: {attr.nbytes*1e-3:.3f}Kb\")\n",
    "    \n",
    "        if isinstance(attr,list) and isinstance(attr[0],(np.ndarray,cp.ndarray)):\n",
    "            \n",
    "            size_entering_loop = size_on_device\n",
    "            for el in attr:\n",
    "                if get_array_module(el) is not np:\n",
    "                    size_on_device += el.nbytes\n",
    "                    \n",
    "            print(f\"Attribute {att}: {(size_on_device-size_entering_loop)*1e-3:.3f}Kb\")\n",
    "    \n",
    "    print(f\"\\nEstimated size on device: {size_on_device*1e-6:.3f}Mb\")\n",
    "    print(f\"Reported size on device: {mempool.used_bytes()*1e-6:.3f}Mb\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def release_class_gpu_memory(clear_fft_cache = True):\n",
    "\n",
    "    if clear_fft_cache is True:\n",
    "        cache.clear()\n",
    "        \n",
    "    mempool.free_all_blocks()\n",
    "    pinned_mempool.free_all_blocks()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5a642-ec81-4d1e-a399-2168e7915c5a",
   "metadata": {},
   "source": [
    "### CPU-only calculation\n",
    "\n",
    "To estabish a baseline, we perform a CPU-only calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e071f06-509e-4bcd-b8ab-55f575457cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated size on device: 0.000Mb\n",
      "Reported size on device: 0.000Mb\n"
     ]
    }
   ],
   "source": [
    "ptycho = py4DSTEM.process.phase.SingleslicePtychography(\n",
    "    datacube=dataset, \n",
    "    energy=80e3, \n",
    "    defocus=500, \n",
    "    vacuum_probe_intensity=probe.data,\n",
    "    verbose = False,\n",
    "    device='cpu', # default\n",
    ").preprocess(\n",
    "    plot_rotation=False,\n",
    "    plot_center_of_mass = False,\n",
    "    plot_probe_overlaps = False,\n",
    "    vectorized_com_calculation = False, # disable default CoM vectorized calc\n",
    "    store_initial_arrays= False, # don't store arrays necessary for reset=True\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224edfb6-1ed5-44cf-9d38-d2331507aeea",
   "metadata": {},
   "source": [
    "As expected, preprocessing with `device='cpu'` uses no GPU memory.  \n",
    "Also notice two additional flags to help with memory issues:\n",
    "- `vectorized_com_calculation=False` &rarr; computes the otherwise memory-expensive CoM calculation using a for-loop\n",
    "- `store_initial_arrays=False` &rarr; doesn't store the arrays necessary to restart the calculation\n",
    "\n",
    "`reconstruct` will similarly work directly on the CPU and use no GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd59e20-927b-446e-ad92-90df37aaeaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing object and probe: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.12s/ iter]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated size on device: 0.000Mb\n",
      "Reported size on device: 0.000Mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ptycho = ptycho.reconstruct(\n",
    "    seed_random=0, \n",
    "    num_iter = 2,\n",
    "    max_batch_size=512,\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d8107-ac11-4485-b68c-6ee89a224d8f",
   "metadata": {},
   "source": [
    "### Expensive calculations on GPU\n",
    "\n",
    "The next step is to store the 4D-data and other necessary utility arrays on the CPU, but perform the actual computations on the GPU by \"streaming\" the necessary data inside the loop. This can be enabled by specifying `device='gpu'` and `storage='cpu'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d0710e-2795-494d-ad8c-523755379f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute _known_aberrations_array: 320.000Kb\n",
      "Attribute _object: 11139.200Kb\n",
      "Attribute _probe: 320.000Kb\n",
      "\n",
      "Estimated size on device: 11.779Mb\n",
      "Reported size on device: 11.780Mb\n"
     ]
    }
   ],
   "source": [
    "ptycho = None\n",
    "release_class_gpu_memory()\n",
    "\n",
    "ptycho = py4DSTEM.process.phase.SingleslicePtychography(\n",
    "    datacube=dataset, \n",
    "    energy=80e3, \n",
    "    defocus=500, \n",
    "    vacuum_probe_intensity=probe.data,\n",
    "    verbose = False,\n",
    "    device='gpu', # perform calculations on the GPU\n",
    "    storage='cpu', # store data and other utility arrays on CPU\n",
    ").preprocess(\n",
    "    plot_rotation=False,\n",
    "    plot_center_of_mass = False,\n",
    "    plot_probe_overlaps = False,\n",
    "    vectorized_com_calculation = False, # disable default CoM vectorized calc\n",
    "    store_initial_arrays= False, # don't store arrays necessary for reset=True\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61370319-1f15-4b47-bae4-be0f47287414",
   "metadata": {},
   "source": [
    "As we can see this only stores the minimum necessary arrays on the GPU (notably the object and probe arrays), and releases all other intermediate memory after execution (seen by the estimated size above matches the reported size by cupy).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3760a361-b8eb-44c8-860b-fcae141aa386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing object and probe: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.54 iter/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute _known_aberrations_array: 320.000Kb\n",
      "Attribute _object: 11139.200Kb\n",
      "Attribute _probe: 320.000Kb\n",
      "\n",
      "Estimated size on device: 11.779Mb\n",
      "Reported size on device: 11.780Mb\n"
     ]
    }
   ],
   "source": [
    "ptycho = ptycho.reconstruct(\n",
    "    seed_random=0, \n",
    "    num_iter = 2,\n",
    "    max_batch_size=512,\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b86008-4671-43f8-af37-6634e1ffaaee",
   "metadata": {},
   "source": [
    "### FFT Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f035cb-dec8-4cd4-8d48-7c07ca979326",
   "metadata": {},
   "source": [
    "In addition to intermediate arrays which are deallocated and cleared above, the other big culprit in the GPU storing more arrays than presumably requested is cupy's default FFT plan caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c7f500-5cf5-49b5-b644-1cf65ed07aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute _known_aberrations_array: 320.000Kb\n",
      "Attribute _object: 11139.200Kb\n",
      "Attribute _probe: 320.000Kb\n",
      "\n",
      "Estimated size on device: 11.779Mb\n",
      "Reported size on device: 11.789Mb\n"
     ]
    }
   ],
   "source": [
    "ptycho = None\n",
    "release_class_gpu_memory(clear_fft_cache=False)\n",
    "\n",
    "ptycho = py4DSTEM.process.phase.SingleslicePtychography(\n",
    "    datacube=dataset, \n",
    "    energy=80e3, \n",
    "    defocus=500, \n",
    "    vacuum_probe_intensity=probe.data,\n",
    "    verbose = False,\n",
    "    device='gpu', # perform calculations on the GPU\n",
    "    storage='cpu', # store data and other utility arrays on CPU\n",
    "    clear_fft_cache = False, # don't clear FFT cache at the end\n",
    ").preprocess(\n",
    "    plot_rotation = False,\n",
    "    plot_center_of_mass = False,\n",
    "    plot_probe_overlaps = False,\n",
    "    vectorized_com_calculation = False, # disable default CoM vectorized calc\n",
    "    store_initial_arrays = False, # don't store arrays necessary for reset=True\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729712d-337e-4b7c-838b-3ef6dbafe98c",
   "metadata": {},
   "source": [
    "Notice that when we don't specify `clear_fft_cache=True` (on by default), the GPU reports some additional memory usage. These are the cached FFT plans, which you can inspect as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64eeadbd-66f0-4970-8650-175f892cb6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- cuFFT plan cache (device 0) -------------------\n",
      "cache enabled? True\n",
      "current / max size   : 2 / 16 (counts)\n",
      "current / max memsize: 9728 / (unlimited) (bytes)\n",
      "hits / misses: 2 / 2 (counts)\n",
      "\n",
      "cached plans (most recently used first):\n",
      "key: ((200, 200), (200, 200), 1, 40000, (200, 200), 1, 40000, 41, 1089, 'C', 2, None), plan type: PlanNd, memory usage: 9216\n",
      "key: ((200, 200), (200, 200), 1, 1, (200, 200), 1, 1, 41, 1, 'C', 1, None), plan type: PlanNd, memory usage: 512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache.show_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611379bc-bffd-43b1-94bb-6836eb4da808",
   "metadata": {},
   "source": [
    "By default, we enable cupy's default FFT plan caching within each function call (since it is quite performant), but clear the cache at the end of `preprocess`, `reconstruct`, and `visualize`. An even more aggresive solution to allow better memory management still is to limit the cache size, or disable it completely (by specifying a size of zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d65acb-7695-43e9-9993-1aaa813435ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing object and probe: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.78 iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute _known_aberrations_array: 320.000Kb\n",
      "Attribute _object: 11139.200Kb\n",
      "Attribute _probe: 320.000Kb\n",
      "\n",
      "Estimated size on device: 11.779Mb\n",
      "Reported size on device: 11.780Mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cache.set_size(0)\n",
    "\n",
    "ptycho = None\n",
    "release_class_gpu_memory()\n",
    "\n",
    "ptycho = py4DSTEM.process.phase.SingleslicePtychography(\n",
    "    datacube=dataset, \n",
    "    energy=80e3, \n",
    "    defocus=500, \n",
    "    vacuum_probe_intensity=probe.data,\n",
    "    verbose = False,\n",
    "    device='gpu', # perform calculations on the GPU\n",
    "    storage='cpu', # store data and other utility arrays on CPU\n",
    "    clear_fft_cache=False, # for demonstration purposes that cache is not used\n",
    ").preprocess(\n",
    "    plot_rotation=False,\n",
    "    plot_center_of_mass = False,\n",
    "    plot_probe_overlaps = False,\n",
    "    vectorized_com_calculation = False, # disable default CoM vectorized calc\n",
    "    store_initial_arrays= False, # don't store arrays necessary for reset=True\n",
    ").reconstruct(\n",
    "    seed_random=0, \n",
    "    num_iter = 2,\n",
    "    max_batch_size=512,\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfab843-750e-4dfb-8fdf-5f4b37100325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- cuFFT plan cache (device 0) -------------------\n",
      "cache enabled? False\n",
      "current / max size   : 0 / 0 (counts)\n",
      "current / max memsize: 0 / (unlimited) (bytes)\n",
      "hits / misses: 0 / 0 (counts)\n",
      "\n",
      "cached plans (most recently used first):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cache.show_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe39ff8-df39-42bc-9cb0-013e606f0648",
   "metadata": {},
   "source": [
    "### Store/compute all arrays on GPU  \n",
    "If memory is not a concern, then you can skip the `storage='cpu'` flag and store all arrays on the GPU for the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e50f87-704a-4cab-95a6-9ea8d75c0471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing object and probe: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.46 iter/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute _amplitudes: 174240.000Kb\n",
      "Attribute _com_fitted_x: 4.356Kb\n",
      "Attribute _com_fitted_y: 4.356Kb\n",
      "Attribute _com_measured_x: 4.356Kb\n",
      "Attribute _com_measured_y: 4.356Kb\n",
      "Attribute _com_normalized_x: 4.356Kb\n",
      "Attribute _com_normalized_y: 4.356Kb\n",
      "Attribute _com_x: 4.356Kb\n",
      "Attribute _com_y: 4.356Kb\n",
      "Attribute _known_aberrations_array: 320.000Kb\n",
      "Attribute _object: 11139.200Kb\n",
      "Attribute _positions_initial: 8.712Kb\n",
      "Attribute _positions_px: 8.712Kb\n",
      "Attribute _positions_px_initial: 8.712Kb\n",
      "Attribute _positions_px_initial_com: 0.008Kb\n",
      "Attribute _probe: 320.000Kb\n",
      "\n",
      "Estimated size on device: 186.080Mb\n",
      "Reported size on device: 186.085Mb\n"
     ]
    }
   ],
   "source": [
    "cache.set_size(16) # restore to default caching\n",
    "\n",
    "ptycho = None\n",
    "release_class_gpu_memory()\n",
    "\n",
    "ptycho = py4DSTEM.process.phase.SingleslicePtychography(\n",
    "    datacube=dataset, \n",
    "    energy=80e3, \n",
    "    defocus=500, \n",
    "    vacuum_probe_intensity=probe.data,\n",
    "    verbose = False,\n",
    "    device='gpu', # perform calculations on the GPU\n",
    "    storage='gpu', # can be omited, defaults to 'device'\n",
    ").preprocess(\n",
    "    plot_rotation=False,\n",
    "    plot_center_of_mass = False,\n",
    "    plot_probe_overlaps = False,\n",
    "    vectorized_com_calculation = False, # disable default CoM vectorized calc\n",
    "    store_initial_arrays= False, # don't store arrays necessary for reset=True\n",
    ").reconstruct(\n",
    "    seed_random=0, \n",
    "    num_iter = 2,\n",
    "    max_batch_size=512,\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88b70b-d81f-418a-a2a2-49fa934eafc1",
   "metadata": {},
   "source": [
    "Notice this reports a lot more arrays on the GPU, notably the amplitude data itself - which can be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd09155-9527-4bf9-9b60-fef53d8a1470",
   "metadata": {},
   "source": [
    "### Function-specific `device`\n",
    "\n",
    "Note that you can overwrite the top-level `device` and `clear_fft_cache` attributes set at initialization during each subsequent function call, meaning you can e.g. preprocess your data on the 'cpu' and reconstruct on the 'gpu':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cdb78f8-6b27-4828-9f60-843436f914db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing object and probe: 100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.66 iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute _known_aberrations_array: 320.000Kb\n",
      "Attribute _object: 11139.200Kb\n",
      "Attribute _object_initial: 11139.200Kb\n",
      "Attribute _probe: 640.000Kb\n",
      "Attribute _probe_initial: 640.000Kb\n",
      "Attribute _probe_initial_aperture: 320.000Kb\n",
      "\n",
      "Estimated size on device: 24.198Mb\n",
      "Reported size on device: 24.199Mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ptycho = None\n",
    "release_class_gpu_memory()\n",
    "\n",
    "ptycho = py4DSTEM.process.phase.SingleslicePtychography(\n",
    "    datacube=dataset, \n",
    "    energy=80e3, \n",
    "    defocus=500, \n",
    "    vacuum_probe_intensity=probe.data,\n",
    "    verbose = False,\n",
    "    device='cpu', # preprocess on the CPU\n",
    ").preprocess(\n",
    "    plot_rotation=False,\n",
    "    plot_center_of_mass = False,\n",
    "    plot_probe_overlaps = False,\n",
    "    vectorized_com_calculation = False, # disable default CoM vectorized calc\n",
    ").reconstruct(\n",
    "    seed_random=0, \n",
    "    num_iter = 2,\n",
    "    max_batch_size=512,\n",
    "    device='gpu', # reconstruct on the GPU\n",
    ")\n",
    "\n",
    "report_class_gpu_memory_usage(ptycho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e0bfe-7b48-4b80-bef6-61269055061f",
   "metadata": {},
   "source": [
    "### `max_batch_size`\n",
    "\n",
    "Finally, you can use the `max_batch_size` in both `preprocess` and `reconstruct` to ensure the number of probe-positions streamed to `device` at any given iteration is kept small-enough for your GPU memory to handle.\n",
    "\n",
    "Note: this option currently defaults to \"mini-batch\" behaviour in stochastic gradient descent, meaning the object and probes will be updated per batch, not per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604fcf8-5a66-42ee-8384-5303b4081e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
