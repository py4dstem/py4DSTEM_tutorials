{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51df62b",
   "metadata": {},
   "source": [
    "# Prefiltering tutorial notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e66c",
   "metadata": {},
   "source": [
    "This notebook performs the prefiltering of the 4D-STEM dataset of the Au nanocrystal growth experiment in a liquid cell, as described in: \n",
    "\n",
    "XXXXX (paper)\n",
    "\n",
    "\n",
    "Experimental data was collected by Serin Lee (serinl@stanford.edu) and Andrew Barnum ()\n",
    "\n",
    "This tutorial was created by: \n",
    "- Serin lee (serinl@stanford.edu)\n",
    "- Stephanie Ribet (sribet@lbl.gov)\n",
    "- Arthur McCray (amccray@stanford.edu)\n",
    "- Colin Ophus (cophus@stanford.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bbae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d87bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU index 3\n",
      "True\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cupy as cp \n",
    "\n",
    "dev_count = torch.cuda.device_count()\n",
    "GPU_IND = 3\n",
    "if dev_count == 0: \n",
    "    print(\"GPU not available, using cpu only\")\n",
    "    device = \"cpu\"\n",
    "    cp = np \n",
    "else: \n",
    "    device = f\"cuda:{GPU_IND}\"\n",
    "    print(f\"Using GPU index {GPU_IND}\")\n",
    "    cp.cuda.Device(GPU_IND).use() \n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb661981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cupyx.jit.rawkernel is experimental. The interface can change in the future.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import py4DSTEM as p4\n",
    "import pymatgen\n",
    "from scipy.ndimage import binary_erosion\n",
    "from py4DSTEM.process.utils import tqdmnd\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from py4DSTEM.process.utils.cluster import Cluster\n",
    "from datetime import datetime\n",
    "from pathlib import Path \n",
    "from cupyx.scipy.ndimage import gaussian_filter as cu_gaussian_filter\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_triangle\n",
    "import scipy.ndimage as ndi \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b7372",
   "metadata": {},
   "source": [
    "# Example Dataset (Zenodo)\n",
    "\n",
    "This tutorial uses a publicly available 4D-STEM dataset archived on Zenodo.\n",
    "\n",
    "**Dataset DOI:**  \n",
    "[10.5281/zenodo.XXXXXXX](https://doi.org/10.5281/zenodo.XXXXXXX)\n",
    "\n",
    "**Title:**  \n",
    "*Example 4D-STEM dataset for the clustering workflow described in \"<Your Paper Title>\"*\n",
    "\n",
    "**Description:**  \n",
    "This dataset contains a prefiltered and compressed 4D-STEM data used to demonstrate\n",
    "the clustering workflow. It is intended for use with this tutorial notebook and the associated publication.\n",
    "\n",
    "You can download the dataset automatically through the notebook (see code cell below),\n",
    "or manually via the DOI link.\n",
    "\n",
    "**How to cite this dataset:**  \n",
    "If you use the data, please cite:\n",
    "\n",
    "> S. Lee, <coauthors>, “Example 4D-STEM dataset for the clustering workflow,”  \n",
    "> Zenodo (2025). https://doi.org/10.5281/zenodo.XXXXXXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e60698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.6\n",
      "0.14.19\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(p4.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e703d",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130367d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "ZENODO_RECORD_ID = \"XXXXXXX\"   # replace with Zenodo record ID\n",
    "FILENAME = \"example_4dstem_dataset\"   # replace with your filename\n",
    "\n",
    "url = f\"https://zenodo.org/records/{ZENODO_RECORD_ID}/files/{FILENAME}?download=1\"\n",
    "out_path = DATA_DIR / FILENAME\n",
    "\n",
    "if not out_path.exists():\n",
    "    print(f\"Downloading {FILENAME} from Zenodo...\")\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(f\"Using existing file: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d85da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "semiangle_convergence_mrad = 0.622\n",
    "energy = 300e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=p4.import_file(\n",
    "    out_path,\n",
    "    filetype = \"arina\",\n",
    "    scan_width = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475b01d",
   "metadata": {},
   "source": [
    "# Median filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc004b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.get_dp_mean();\n",
    "median_mask = dataset.tree('dp_mean').data > 6e4\n",
    "dataset=dataset.median_filter_masked_pixels(median_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf8403",
   "metadata": {},
   "source": [
    "# Correlative Prefiltering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtype = np.float32\n",
    "corr_range = np.array((0.85, 0.95), dtype=np.float32)\n",
    "edge = 12  \n",
    "bound = 4\n",
    "sigma = 3\n",
    "batch_rows = 4\n",
    "\n",
    "\n",
    "EPS = 1e-12  # FIX: global small epsilon for safe divisions\n",
    "x = np.arange(dataset.shape[2]).astype('float')\n",
    "y = np.arange(dataset.shape[3]).astype('float')\n",
    "x -= np.mean(x)\n",
    "y -= np.mean(y)\n",
    "weight = np.sqrt(x[:,None]**2 + y[None,:]**2)\n",
    "weight /= np.sqrt(dataset.shape[2]*dataset.shape[3])\n",
    "weight **= 2\n",
    "edge = 12\n",
    "weight[:edge,:] = 0.0\n",
    "weight[:,:edge] = 0.0\n",
    "weight[-edge:,:] = 0.0\n",
    "weight[:,-edge:] = 0.0\n",
    "\n",
    "# FIX: NaN-guard + safe weight_sum (could be 0 if image is smaller than 2*edge)\n",
    "weight = np.nan_to_num(weight, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "weight_sum = float(np.sum(weight))\n",
    "if weight_sum <= 0.0:\n",
    "    # Fallback: uniform weights inside the non-edge region\n",
    "    weight[...] = 0.0\n",
    "    core = (slice(edge, max(dataset.shape[2] - edge, edge + 1)),\n",
    "            slice(edge, max(dataset.shape[3] - edge, edge + 1)))\n",
    "    weight[core] = 1.0\n",
    "    weight_sum = float(np.sum(weight))\n",
    "# Final clamp\n",
    "weight_sum = max(weight_sum, EPS)\n",
    "\n",
    "\n",
    "r = np.arange(-bound,bound+1)\n",
    "yy,xx = np.meshgrid(r,r)\n",
    "footprint = xx**2 + yy**2 <= (bound+0.5)**2\n",
    "dx = xx[footprint]\n",
    "dy = yy[footprint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25132c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntime=datetime.now()\n",
    "print(\"Starting guassian filtering loop\")\n",
    "\n",
    "blurred = np.empty_like(dataset.data, dtype=np.float32)\n",
    "\n",
    "for i0 in range(0, dataset.shape[0], batch_rows):\n",
    "    i1 = min(i0 + batch_rows, dataset.shape[0])\n",
    "    # Slice a small block (i0:i1, :, :, :)\n",
    "    block = dataset.data[i0:i1].astype(np.float32, copy=False)\n",
    "\n",
    "    # Move to GPU\n",
    "    block_gpu = cp.asarray(block, order='C')  # shape (B, , Qx, Qy)\n",
    "\n",
    "    # Use a simple boundary mode; 'nearest' or 'reflect' are typical. Match your CPU choice if needed.\n",
    "    blurred_gpu = cu_gaussian_filter(block_gpu, sigma=(0, 0, sigma, sigma), mode='reflect')\n",
    "\n",
    "    # Bring back to CPU\n",
    "    blurred[i0:i1] = cp.asnumpy(blurred_gpu)\n",
    "\n",
    "    # Clean up GPU memory between batches\n",
    "    del block_gpu, blurred_gpu\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "print(f\"finished guassian filtering loop: {datetime.now() - ntime}\")\n",
    "\n",
    "# prefiltering\n",
    "ntime=datetime.now()\n",
    "print(\"Starting prefiltering loop...\")\n",
    "\n",
    "blurred = np.nan_to_num(blurred, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "data_new = np.zeros(dataset.data.shape, dtype='float32')  \n",
    "\n",
    "# FIX: safe denominator for corr_range mapping\n",
    "corr_den = float(corr_range[1] - corr_range[0])\n",
    "corr_den = corr_den if abs(corr_den) > EPS else EPS\n",
    "\n",
    "\n",
    "for rx, ry in tqdmnd(\n",
    "    \n",
    "    range(dataset.shape[0]),\n",
    "    range(dataset.shape[1]),\n",
    "):\n",
    "    # ---- reference tile ----\n",
    "    im0 = blurred[rx,ry]\n",
    "\n",
    "    # weighted, mean-removed\n",
    "    wmean0 = np.sum(im0 * weight) / weight_sum   # FIX: safe weight_sum\n",
    "    im0 = (im0 - wmean0) * weight\n",
    "\n",
    "    # FIX: safe L2 normalization\n",
    "    norm0 = float(np.sqrt(np.sum(im0**2)))\n",
    "    im0 /= max(norm0, EPS)\n",
    "\n",
    "    corr = np.zeros(dx.size, dtype=float)\n",
    "\n",
    "    # ---- neighbors ----\n",
    "    for ind in range(dx.size):\n",
    "        xt = rx + dx[ind]\n",
    "        yt = ry + dy[ind]\n",
    "\n",
    "        if (xt >= 1) and (yt >= 1) and (xt < dataset.shape[0]) and (yt < dataset.shape[1]):\n",
    "            im1 =blurred[xt, yt]\n",
    "            wmean1 = np.sum(im1 * weight) / weight_sum     # FIX: safe weight_sum\n",
    "            im1 = (im1 - wmean1) * weight\n",
    "            norm1 = float(np.sqrt(np.sum(im1**2)))         # FIX: safe L2 normalization\n",
    "            im1 /= max(norm1, EPS)\n",
    "\n",
    "            # dot-product correlation (safe because im0, im1 are finite & normalized)\n",
    "            corr[ind] = np.sum(im0 * im1)\n",
    "\n",
    "    # map corr into [0,1] using corr_range, with safe denominator\n",
    "    corr = (corr - corr_range[0]) / corr_den\n",
    "    corr = np.clip(corr, 0.0, 1.0)\n",
    "\n",
    "    # ---- accumulate filtered output ----\n",
    "    data_new[rx, ry] = dataset.data[rx, ry]\n",
    "    for ind in range(dx.size):\n",
    "        if corr[ind] > 1e-3:\n",
    "            xt = rx + dx[ind]\n",
    "            yt = ry + dy[ind]\n",
    "            data_new[rx, ry] += dataset.data[xt, yt] * corr[ind]\n",
    "\n",
    "    # FIX: denominator is already safe ( +1 ), but make sure corr sum is finite\n",
    "    corr_sum = float(np.sum(corr))\n",
    "    if not np.isfinite(corr_sum):\n",
    "        corr_sum = 0.0\n",
    "    data_new[rx, ry] /= (corr_sum + 1.0)\n",
    "print(f\"finished prefiltering loop: {datetime.now() - ntime}\")\n",
    "\n",
    "# saving as compressed h5 file\n",
    "ntime=datetime.now()\n",
    "print(\"Starting npz compressing loop...\")\n",
    "\n",
    "arr = data_new\n",
    "# Step 1: normalize into [0, 1]\n",
    "arr_min, arr_max = float(arr.min()), float(arr.max())\n",
    "if arr_max > arr_min:\n",
    "    arr_norm = (arr - arr_min) / (arr_max - arr_min)\n",
    "else:\n",
    "    arr_norm = np.zeros_like(arr, dtype=np.float32)\n",
    "\n",
    "# Step 2: scale to 0–65535 and cast to uint16\n",
    "arr_uint16 = (arr_norm * 65535).astype(np.uint16)\n",
    "\n",
    "print(f\"finished compressing loop: {datetime.now() - ntime}\")    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py4DSTEM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
